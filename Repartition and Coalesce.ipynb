{"cells":[{"cell_type":"code","source":["#### Coalesce(numPartitions): Decrease number of partitions in the RDD to numPartitions.\nUseful for running operations more efficiently after filtering down a large dataset.\nIt just combine the existing partition to reduce the number of partitions \n# repartition(numPartitions): reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them.\nthis always shuffles all data over the network.\nkey ideas:\n1. operations which can cause a shuffle iclude repartitions like repartition and coalesce.\n2. Repartition(1) and Coalesce(1) can be used to output data in a shuffle  file but Coalesce(1) will perform better as no reshuffle occurs.\n3. number of partition =# of CPU*4\n4. When some empty or light files are produced after the final processing, use coalesce.\n->>> Coalesce is preferred over reparation if we have to reduce the count of partitions in an RDD\n->>> If data is skewness, then repartition is preferred but data is not skewed then repartition is preferred\n->>> As a thumb, each output file should be around 128MB or 256MB,\nif that is not the case, use coalesce in the stage of the pipeline where partitions in the output have much lesser or o data at all.\n->>> If we will perform some filter operations or other operations that \nwill trigger empty partition then at the time coalesce is called better \n->>> Avoiding shuffles: Shuffle are quite expensive, when we have joined\n  a bigger dataset quite frequently with some smaller dataset which are \n  coming at a higher speed and time frame then, we can cache the bigger RDD\n  and whenever a new data comes in you join with (when joining a bigger dataset\n  frequently with smaller datasets on the same key, look for opportunity to avoid shuffle) with the cached RDD, \n  when we cached the RDD in the memory, then reading the file again and again, you do not avoid shuffle. \n  You cache the RDD and you repartition the RDD as well based on the field over which you would like to join, \n  so we can think like pair RDD, where we can construct the pair RDD with key as the field with which \n  you are going to join and you cache the bigger RDD and re-partition it as well when we do that the smaller\n  files that keeps on arriving and they have to be constantly joined with the bigger data sets only the smaller\n  data set and has to be shuffled and the bigger dataset will remain static. \n  Shuffling on the smaller dataset is faster as compared to shuffling both the data sets \n->>> If we have some kind of topN, rank or row number operations, prefer secondary sorting.\n->>> Prefer reduceByKey or combineByKey over groupByKey as the former reduces the amount of data that gets shuffled.\n->>> Avoid collecting larger results in memory – the output of the spark application is not \nrequired to be displayed to the user in totality, you may be having millions of records you \ndon’t want the all the records collected on the driver. You would like to save it \nto the file, but don’t call collect, it leads out of memory on driver. It is always preferred like sample and take \n->>> Avoid unnecessary actions. Each action triggers one spark job, we can cache \nthe RDD if multiple actions are needed. We can cache or repartition a bigger RDD \nif it has to be constantly joined with smaller datasets. \nIf we have resources at your disposal, you can increase the availability \nof each partition by replicating it, you can specify underscore2, \nin your storage level like memory only, underscore2 memory and disk and so on. \nEven if one machine goes down the cache data is present on some other machine \n->>> If we have to collect metrics, metrics which can be collected via accumulators can avoid additional actions, \n->>> If we need to check if an RDD is empty or not\n->>> Instead of using if rdd.count() > 0\n->>> Use, if len(rdd.take(1)) > 0\n->>> Count will scan entire RDD, whereas take() will just read only those partitions from where results can be retrieved\n\n## the repartition algorith does a full shuffle and creates new partitions with data that's distributed evenly. (stack Overflow)\n## Coalesce avoids a full shuffle. \nIf it is known that the number is decreasing then the executor can safely keep data on the minimum number of partitions, \nonly moving the data off the extra nodes, onto the nodes that we kept.(stackoverflow)\n## Coalesce uses existing partitions to minimize the amount of data that's shuffled. Repartition creates new partitions \nand does a full shuffle. Coalesce results in partitions with different amounts of data 9sometimes partitions \nthat have much different sizes) and repartition results in roughly equal sized partitions.(Stackoverflow)\n## coalesce may run faster than repartition, but unequal sized partitions are generally slower to work with than equal\nsized partitions. You will usually need to repartition datasets after filtering a large data set.\nHe mentioned extra more that, spark is build for to work for equal sized partitions. 9stackoverflow)\n# repartition will ignore existing partitions and create new ones. So you can use it to fix data skew. \nYou can mention partition keys to define the distribution. Data skew is one of the biggest problems in the\n'big data' problem space. Other thing is in repartition by providing the number of partitions,\nit tries to redistribute the data uniformly on all the partitions while in case of Coalesce \nwe could still have skew data in some cases.(stackoverflow)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62f22da1-ee34-4379-bbfb-ad62eaa68743"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["df = spark.read.csv(\"dbfs:/FileStore/shared_uploads/sudippandit99@gmail.com/finance.xlsx\", header =True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5db18077-8c9f-439b-b2be-df5f93719a90"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94ef8148-6662-4da3-ab44-38126e283758"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[12]: 1","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[12]: 1"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["df1 = df.repartition(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a84fbe3f-2203-4542-a642-62d6dd49716f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df1.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a0399f62-d545-4ab7-9d8b-d6483a97b1a9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[16]: 591","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[16]: 591"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["df2 = df.coalesce(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a525b51d-c578-4bbc-b213-7aeb26b939bc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df2.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d347f8b3-ffeb-4977-bf9e-de8206326b28"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[19]: 591","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[19]: 591"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"2021-11-28 - DBFS Example","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":932413955475192}},"nbformat":4,"nbformat_minor":0}
