{"cells":[{"cell_type":"code","source":["#### Coalesce(numPartitions): Decrease number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset. It just combine the existing partition to reduce the number of partitions \n# repartition(numPartitions): reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. this always shuffles all data over the network.\nkey ideas:\n1. operations which can cause a shuffle iclude repartitions like repartition and coalesce.\n2. Repartition(1) and Coalesce(1) can be used to output data in a shuffle  file but Coalesce(1) will perform better as no reshuffle occurs.\n3. number of partition =# of CPU*4\n4. When some empty or light files are produced after the final processing, use coalesce.\n->>> Coalesce is preferred over reparation if we have to reduce the count of partitions in an RDD\n->>> If data is skewness, then repartition is preferred but data is not skewed then repartition is preferred\n->>> As a thumb, each output file should be around 128MB or 256MB, if that is not the case, use coalesce in the stage of the pipeline where partitions in the output have much lesser or o data at all.\n->>> If we will perform some filter operations or other operations that will trigger empty partition then at the time coalesce is called better \n->>> Avoiding shuffles: Shuffle are quite expensive, when we have joined a bigger dataset quite frequently with some smaller dataset which are coming at a higher speed and time frame then, we can cache the bigger RDD and whenever a new data comes in you join with (when joining a bigger dataset frequently with smaller datasets on the same key, look for opportunity to avoid shuffle) with the cached RDD, when we cached the RDD in the memory, then reading the file again and again, you do not avoid shuffle. You cache the RDD and you repartition the RDD as well based on the field over which you would like to join, so we can think like pair RDD, where we can construct the pair RDD with key as the field with which you are going to join and you cache the bigger RDD and re-partition it as well when we do that the smaller files that keeps on arriving and they have to be constantly joined with the bigger data sets only the smaller data set and has to be shuffled and the bigger dataset will remain static. Shuffling on the smaller dataset is faster as compared to shuffling both the data sets \n->>> If we have some kind of topN, rank or row number operations, prefer secondary sorting.\n->>> Prefer reduceByKey or combineByKey over groupByKey as the former reduces the amount of data that gets shuffled.\n->>> Avoid collecting larger results in memory – the output of the spark application is not required to be displayed to the user in totality, you may be having millions of records you don’t want the all the records collected on the driver. You would like to save it to the file, but don’t call collect, it leads out of memory on driver. It is always preferred like sample and take \n->>> Avoid unnecessary actions. Each action triggers one spark job, we can cache the RDD if multiple actions are needed. We can cache or repartition a bigger RDD if it has to be constantly joined with smaller datasets. If we have resources at your disposal, you can increase the availability of each partition by replicating it, you can specify underscore2, in your storage level like memory only, underscore2 memory and disk and so on. Even if one machine goes down the cache data is present on some other machine \n->>> If we have to collect metrics, metrics which can be collected via accumulators can avoid additional actions, \n->>> If we need to check if an RDD is empty or not\n->>> Instead of using if rdd.count() > 0\n->>> Use, if len(rdd.take(1)) > 0\n->>> Count will scan entire RDD, whereas take() will just read only those partitions from where results can be retrieved\n\n## the repartition algorith does a full shuffle and creates new partitions with data that's distributed evenly. (stack Overflow)\n## Coalesce avoids a full shuffle. If it is known that the number is decreasing then the executor can safely keep data on the minimum number of partitions, only moving the data off the extra nodes, onto the nodes that we kept.(stackoverflow)\n## Coalesce uses existing partitions to minimize the amount of data that's shuffled. Repartition creates new partitions and does a full shuffle. Coalesce results in partitions with different amounts of data 9sometimes partitions that have much different sizes) and repartition results in roughly equal sized partitions.(Stackoverflow)\n## coalesce may run faster than repartition, but unequal sized partitions are generally slower to work with than equal sized partitions. You will usually need to repartition datasets after filtering a large data set. he mentioned extra more that, spark is build for to work for equal sized partitions. 9stackoverflow)\n# repartition will ignore existing partitions and create new ones. So you can use it to fix data skew. You can mention partition keys to define the distribution. Data skew is one of the biggest problems in the 'big data' problem space. Other thing is in repartition by providing the number of partitions, it tries to redistribute the data uniformly on all the partitions while in case of Coalesce we could still have skew data in some cases.(stackoverflow)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62f22da1-ee34-4379-bbfb-ad62eaa68743"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["df = spark.read.csv(\"dbfs:/FileStore/shared_uploads/sudippandit99@gmail.com/finance.xlsx\", header =True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5db18077-8c9f-439b-b2be-df5f93719a90"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94ef8148-6662-4da3-ab44-38126e283758"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[12]: 1","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[12]: 1"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["df1 = df.repartition(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a84fbe3f-2203-4542-a642-62d6dd49716f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df1.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a0399f62-d545-4ab7-9d8b-d6483a97b1a9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[16]: 591","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[16]: 591"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["df2 = df.coalesce(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a525b51d-c578-4bbc-b213-7aeb26b939bc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df2.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d347f8b3-ffeb-4977-bf9e-de8206326b28"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[19]: 591","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[19]: 591"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"2021-11-28 - DBFS Example","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":932413955475192}},"nbformat":4,"nbformat_minor":0}
# Spark uses the internal hash partitioning Schema to split the data into these smaller chunks.
-> we can use the rdd.glom() to display the partitions in a list
-> rdd.glom().collect()

# to control the number of bytes a single partition can hold: 
-> the number of partitions is determined by spark.sql.files.maxPartitionBytes parameter, which is set to 128MB by default
spark.conf.get("spark.sql.files.maxPartitionBytes") -> 128MB by default(Reference: datanoon.com)

#### repartition vs coalesce:
1) when we want our output partitions to be equally distributed chunks
2) increase the number of partitions
3) perform a shuffle of the data and create partitions

coalesce is generally used wehn we want to decrease the number of partitions

from pyspark.sql.types import IntegerType
df = spark.createdataFrame(range(10), IntegerType())
df.coalesce(1).write.format("csv").mode('overwrite').save('path/output1')
(The above method is not good which creates the slowing down process and degrading the performance)
-> This creates a overhead on the single node
-> So, in case our data is not  skewed and we need to lower down the parttions, then  coalesce might be the good operation in spark.
-> But, our data is huge skewed among the partitions and then repartion may be the good method to increase or decrease the partitions. But, we need to keep on our mind that it increase the shuffle operations which also degrades the performance in real word problems with a huge datasets.

